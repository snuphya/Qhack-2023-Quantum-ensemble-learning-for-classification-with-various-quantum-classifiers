{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset load\n",
    "import pandas as pd\n",
    "data = pd.read_csv('dataset/haberman.csv')\n",
    "# Dataset split into trainset testset 5:1 ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "trainset, testset = train_test_split(data,test_size=0.2,random_state=1234, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into x_train, y_train, x_test, y_test \n",
    "# Changing class value from 1, 2 to 0, 1\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for i in range(len(trainset)):\n",
    "    x_train.append(list(trainset.iloc[i, 0:3]))\n",
    "    y_train.append(list(trainset.iloc[i, 3:4])[0])\n",
    "\n",
    "x_test, y_test = [], []\n",
    "\n",
    "for i in range(len(testset)):\n",
    "    x_test.append(list(testset.iloc[i, 0:3]))\n",
    "    y_test.append(list(testset.iloc[i, 3:4])[0])\n",
    "    \n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i] == 2:\n",
    "        y_train[i] = 1\n",
    "    else:\n",
    "        y_train[i] = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] == 2:\n",
    "        y_test[i] = 1\n",
    "    else:\n",
    "        y_test[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data scaling with MaxAbsScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "scaler = MaxAbsScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train_scaled = scaler.transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data re uploading classifier with pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define single qubit data reuploading circuit, both circuits below are same but difference is that one returns probs the other returns state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "devc = qml.device(\"default.qubit\", wires=1, shots=None)\n",
    "@qml.qnode(devc, interface=\"torch\")\n",
    "def compact_circ2(params, x, num_layer):\n",
    "    phi = []\n",
    "    num_input = len(x)\n",
    "    #alpha = params[0:2]\n",
    "    weight = params[2:2+num_input*num_layer].reshape(num_layer,num_input)\n",
    "    bias = params[2+num_input*num_layer:len(params)].reshape(num_layer,3)\n",
    "    for i in range(num_layer):\n",
    "        phi.append(torch.tanh(torch.inner(weight[i], x)))\n",
    "        \n",
    "    if num_layer == 1:\n",
    "        qml.Rot(phi[0]+np.pi*torch.tanh(bias[0][0]), phi[0]+np.pi*torch.tanh(bias[0][1]), phi[0]+np.pi*torch.tanh(bias[0][2]), wires=0)\n",
    "    else:\n",
    "        qml.RZ(phi[0]+np.pi*torch.tanh(bias[0][2]), wires=0)\n",
    "        qml.RY(phi[0]+np.pi*torch.tanh(bias[0][1]), wires=0)\n",
    "        for j in range(num_layer-1):\n",
    "            qml.RZ(phi[j]+np.pi*torch.tanh(bias[j][2])+phi[j+1]+np.pi*torch.tanh(bias[j+1][0]), wires=0)\n",
    "            qml.RY(phi[j]+np.pi*torch.tanh(bias[j][1]), wires=0)\n",
    "            \n",
    "        qml.RZ(phi[num_layer-1]+np.pi*torch.tanh(bias[num_layer-1][2]), wires=0)\n",
    "    return qml.probs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "devc = qml.device(\"default.qubit\", wires=1, shots=None)\n",
    "@qml.qnode(devc, interface=\"torch\")\n",
    "def compact_circ(params, x, num_layer):\n",
    "    phi = []\n",
    "    num_input = len(x)\n",
    "    #alpha = params[0:2]\n",
    "    weight = params[2:2+num_input*num_layer].reshape(num_layer,num_input)\n",
    "    bias = params[2+num_input*num_layer:len(params)].reshape(num_layer,3)\n",
    "    for i in range(num_layer):\n",
    "        phi.append(torch.tanh(torch.inner(weight[i], x)))\n",
    "        \n",
    "    if num_layer == 1:\n",
    "        qml.Rot(phi[0]+np.pi*torch.tanh(bias[0][0]), phi[0]+np.pi*torch.tanh(bias[0][1]), phi[0]+np.pi*torch.tanh(bias[0][2]), wires=0)\n",
    "    else:\n",
    "        qml.RZ(phi[0]+np.pi*torch.tanh(bias[0][2]), wires=0)\n",
    "        qml.RY(phi[0]+np.pi*torch.tanh(bias[0][1]), wires=0)\n",
    "        for j in range(num_layer-1):\n",
    "            qml.RZ(phi[j]+np.pi*torch.tanh(bias[j][2])+phi[j+1]+np.pi*torch.tanh(bias[j+1][0]), wires=0)\n",
    "            qml.RY(phi[j]+np.pi*torch.tanh(bias[j][1]), wires=0)\n",
    "            \n",
    "        qml.RZ(phi[num_layer-1]+np.pi*torch.tanh(bias[num_layer-1][2]), wires=0)\n",
    "    return qml.state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define simple cost function, weighted fidelity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple cost function\n",
    "def cost(params,x,y, num_layer):\n",
    "    los = 0\n",
    "    for i in range(len(x)):\n",
    "        res = compact_circ2(params,x[i], num_layer)\n",
    "        fidelity = (res[1] - y[i])**(2) + (res[0]-(1-y[i]))**2\n",
    "        los += fidelity\n",
    "    return los / (len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define some functions to construct weighted fidelity cost function\n",
    "def label_state(params, x, num_layer):\n",
    "\n",
    "    res = compact_circ2(params, x, num_layer).detach().numpy()\n",
    "\n",
    "    if res[0] >= res[1]:\n",
    "        state = [1,0]\n",
    "    else:\n",
    "        state = [0,1]\n",
    "    return state\n",
    "\n",
    "def state_gen(params, x,y, num_layer): \n",
    "    zero_res, one_res = [], []\n",
    "    zero_label, one_label = [], []\n",
    "    zero_y, one_y = [], []\n",
    "    for i in range(len(x)):\n",
    "        if label_state(params, x[i], num_layer) == [1,0]:\n",
    "            zero_res.append(compact_circ(params, x[i], num_layer))\n",
    "            zero_label.append(torch.tensor([1,0]))\n",
    "            y_label = torch.tensor([1-y[i], y[i]])\n",
    "            zero_y.append(y_label)\n",
    "        elif label_state(params, x[i], num_layer) == [0,1]:\n",
    "            one_res.append(compact_circ(params, x[i], num_layer))\n",
    "            one_label.append(torch.tensor([0,1]))\n",
    "            y_label = torch.tensor([1-y[i], y[i]])\n",
    "            one_y.append(y_label)\n",
    "    if one_res == []:\n",
    "        b,d,e = one_res, one_label, one_y\n",
    "    else:\n",
    "        b = torch.stack(one_res, dim=0)\n",
    "        d = torch.stack(one_label, dim=0).type(torch.complex128)\n",
    "        e = torch.stack(one_y, dim=0).type(torch.complex128)\n",
    "    if zero_res == []:\n",
    "        a,c,f = zero_res, zero_label, zero_y\n",
    "    else:\n",
    "        a = torch.stack(zero_res, dim=0)\n",
    "        c = torch.stack(zero_label, dim=0).type(torch.complex128)\n",
    "        f = torch.stack(zero_y, dim=0).type(torch.complex128)\n",
    "    return a,b,c,d,e,f\n",
    "\n",
    "# weighted fidelity cost fn\n",
    "def weighted_fidelity_cost(params, x, y, num_layer):\n",
    "            \n",
    "    alpha = params[0:2]\n",
    "    zero_res, one_res, zero_label, one_label, one_y, zero_y = state_gen(params, x,y, num_layer)\n",
    "    weighted_fi = 0\n",
    "    for j in range(len(zero_res)):\n",
    "        weighted_fi += (alpha[0]*torch.inner(zero_label[j], zero_res[j])*torch.inner(zero_label[j], zero_res[j]).conj()\n",
    "                        - torch.inner(zero_label[j], zero_y[j])*torch.inner(zero_label[j], zero_y[j]).conj())**2 / len(x) / 2\n",
    "    for k in range(len(one_res)):\n",
    "        weighted_fi += (alpha[1]*torch.inner(one_label[k], one_res[k])*torch.inner(one_label[k], one_res[k]).conj()\n",
    "                        - torch.inner(one_label[k], one_y[k])*torch.inner(one_label[k], one_y[k]).conj())**2 / len(x) / 2\n",
    "        \n",
    "    return weighted_fi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with torch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2459)"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = 0, 1000\n",
    "num_layer = 5\n",
    "x, y = torch.Tensor(x_train_scaled[a:b]), torch.Tensor(y_train[a:b])\n",
    "num_input = len(x[0])\n",
    "params = torch.ones(2+(num_input+3)*num_layer, requires_grad=True)\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can utilise tensorboard to see loss changing according to epochs\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "layer_name = \"{} layers\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "log_dir = f\"dataset/{current_time}-{layer_name.format(num_layer)}/\"\n",
    "writer = SummaryWriter(log_dir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LBFGS optimizer\n",
    "opt = optim.LBFGS([params], lr=1e0)\n",
    "def closure():\n",
    "    opt.zero_grad()\n",
    "    loss = weighted_fidelity_cost(params,x,y,num_layer).type(torch.float64)\n",
    "    loss.backward()\n",
    "    print(loss)\n",
    "    return loss\n",
    "# We just simply set large epochs and stop when it is trained enough\n",
    "for _ in range(10000): \n",
    "    opt.step(closure = closure)  \n",
    "    # writer.add_scalar('loss', closure().type(torch.float64), _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam([params], lr=1e-1)\n",
    "# We just simply set large epochs and stop when it is trained enough\n",
    "for _ in range(10000):\n",
    "    opt.zero_grad()\n",
    "    \n",
    "    loss = weighted_fidelity_cost(params,x,y,num_layer)\n",
    "    # loss = cost(params,x,y,num_layer)\n",
    "    loss.backward()\n",
    "    opt.step()  \n",
    "    # writer.add_scalar('loss', loss.type(torch.float64), _)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classify function\n",
    "def classify(params,x,num_layer):\n",
    "    y_hat = []\n",
    "    for i in range(len(x)):\n",
    "        res = compact_circ2(params, x[i], num_layer).detach().numpy()\n",
    "        if res[0] >= res[1]:\n",
    "            y_hat.append(0)\n",
    "        else:\n",
    "            y_hat.append(1)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3387)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt, yt = torch.Tensor(x_test_scaled), torch.Tensor(y_test)\n",
    "y_hat = classify(params,xt,num_layer)\n",
    "yt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.6935\n",
      "precision : 0.5500\n",
      "recall : 0.5238\n",
      "F1-Score : 0.5366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"accuracy : %.4f\" % accuracy_score(yt,y_hat))\n",
    "print(\"precision : %.4f\" % precision_score(yt, y_hat))\n",
    "print(\"recall : %.4f\" % recall_score(yt, y_hat))\n",
    "print(\"F1-Score : %.4f\" % f1_score(yt, y_hat))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Data re uploading method with Qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AccountProvider for IBMQ(hub='ibm-q', group='open', project='main')>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import qiskit\n",
    "from qiskit_aer import StatevectorSimulator, AerSimulator\n",
    "from qiskit import transpile, assemble, QuantumCircuit, BasicAer, IBMQ\n",
    "from qiskit.visualization import *\n",
    "from qiskit.circuit import Parameter, ParameterVector\n",
    "from scipy.optimize import minimize\n",
    "IBMQ.load_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QC(backend, shots, params, num_layer, x):\n",
    "    \n",
    "    ####### circuit ########\n",
    "    circuit = QuantumCircuit(1,1)\n",
    "\n",
    "    theta = ParameterVector('θ',num_layer*3)\n",
    "    \n",
    "    for i in range(num_layer):\n",
    "        circuit.rz(theta[3*i],0)\n",
    "        circuit.ry(theta[3*i+1],0)\n",
    "        circuit.rz(theta[3*i+2],0)\n",
    "    circuit.measure(0,0)\n",
    "\n",
    "    \n",
    "    ## define parameters  ##\n",
    "    phi = []\n",
    "    num_input = len(x[0])\n",
    "    weight = params[0: num_input*num_layer].reshape(num_layer,num_input)\n",
    "    bias = params[num_input*num_layer:len(params)].reshape(num_layer,3)\n",
    "    for k in range(len(x)):\n",
    "        for i in range(num_layer):\n",
    "            for j in range(num_input):\n",
    "                phi.append(np.inner(weight[i], x[k])+bias[i][j])\n",
    "    phi = np.array(phi).reshape(len(x),3*num_layer).numpy()\n",
    "    ####### job run ########\n",
    "    circs = []\n",
    "    for i in range(len(x)):\n",
    "        circ = circuit.bind_parameters({theta: phi[i]})\n",
    "        circs.append(circ)\n",
    "    \n",
    "    job = backend.run(circs)\n",
    "    result = job.result().get_counts()\n",
    "    prob_0, prob_1 = [], []\n",
    "    for i in range(len(x)):\n",
    "        if '0' in result[i]:\n",
    "            prob_0.append(result[i]['0']/shots)\n",
    "            prob_1.append(1 - result[i]['0']/shots)\n",
    "        else:\n",
    "            prob_0.append(1 - result[i]['1']/shots)\n",
    "            prob_1.append(result[i]['1']/shots)\n",
    "\n",
    "    \n",
    "    return [prob_0, prob_1]\n",
    "\n",
    "def pulse_cost(params, backend, num_layer, shots, x, y):\n",
    "    result = QC(backend, shots, params, num_layer, x)\n",
    "    y_hat = []\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        cost += (result[0][i] - y[i])**(2) + (result[1][i]-(1-y[i]))**2 \n",
    "    return cost / len(x)\n",
    "\n",
    "def pulse_classify(params, backend, num_layer, shots, xt):\n",
    "    result = QC(backend, shots, params, num_layer, xt)\n",
    "    y_hat = []\n",
    "    for i in range(len(xt)):\n",
    "        if result[0][i] > result[1][i]:\n",
    "            y_hat.append(0)\n",
    "        else:\n",
    "            y_hat.append(1)\n",
    "    return y_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "provider = IBMQ.get_provider() # put your hub, group, project\n",
    "# backend = provider.get_backend('ibm_sherbrooke')\n",
    "# backend = AerSimulator.from_backend(backend)\n",
    "backend = StatevectorSimulator\n",
    "backend, shots, num_layer= backend, 1024, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = 0, 10000\n",
    "x, y = torch.Tensor(x_train_scaled), torch.Tensor(y_train)\n",
    "param = np.random.random(6*num_layer).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = minimize(pulse_cost, x0=param, args = (backend, num_layer, shots, x, y) ,method=\"L-BFGS-B\", options={'maxiter':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3387)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c,a = 0, 100000\n",
    "xt, yt = torch.Tensor(x_test_scaled[c:a]), torch.Tensor(y_test[c:a])\n",
    "y_hat2 = pulse_classify(out.x, backend, num_layer,shots, xt)\n",
    "yt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.5323\n",
      "precision : 0.3889\n",
      "recall : 0.6667\n",
      "F1-Score : 0.4912\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print(\"accuracy : %.4f\" % accuracy_score(yt,y_hat2))\n",
    "print(\"precision : %.4f\" % precision_score(yt, y_hat2))\n",
    "print(\"recall : %.4f\" % recall_score(yt, y_hat2))\n",
    "print(\"F1-Score : %.4f\" % f1_score(yt, y_hat2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "830c63090170a2e2dfe34f9f29688475b9737a49e96ffc317f86b78207d49e11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
